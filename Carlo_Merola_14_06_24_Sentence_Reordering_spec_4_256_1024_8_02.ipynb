{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGU5dvhrq_9F"
      },
      "source": [
        "# Carlo Merola - Deep Learning Exam 14_06_24\n",
        "# Student ID: 0001112544\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElNaMbLnRdHR"
      },
      "source": [
        "# Sentence Reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXr4iGUGRms8"
      },
      "source": [
        "The purpose of this project is to take in input a sequence of words corresponding to a random permutation of a given english sentence, and reconstruct the original sentence.\n",
        "\n",
        "The otuput can be either produced in a single shot, or through an iterative (autoregressive) loop generating a single token at a time.\n",
        "\n",
        "\n",
        "CONSTRAINTS:\n",
        "* No pretrained model can be used.\n",
        "* The neural network models should have less the 20M parameters.\n",
        "* No postprocessing should be done (e.g. no beamsearch)\n",
        "* You cannot use additional training data.\n",
        "\n",
        "\n",
        "BONUS PARAMETERS:\n",
        "\n",
        "A bonus of 0-2 points will be attributed to incentivate the adoption of models with a low number of parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ8k-L-WUK7l"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "The dataset is composed by sentences taken from the generics_kb dataset of hugging face. We restricted the vocabolary to the 10K most frequent words, and only took sentences making use of this vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjNyilJrq_9H",
        "outputId": "005280ed-28cf-4e98-bd99-58974b27f5b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nJ02vehGYySk",
        "outputId": "840da875-078e-41b9-cc32-b4d9480e0891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.1 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.19.2 dill-0.3.8 multiprocess-0.70.16 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "807Wk-ir_bDU"
      },
      "source": [
        "Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267,
          "referenced_widgets": [
            "9a9f66d8b9bc4e1c91b652bec4a76569",
            "6706190e612440cda288e23851417305",
            "6a8f063cf8d549379a5d6c2fcb946b22",
            "24f8da3ee602482b8305a056f636cbc4",
            "86ffb50458df45f3a9b924430cecaf66",
            "0bc54c4aac844ed798eefe7e8492beb6",
            "291a6bd34c5146a9860516a5ed510ef0",
            "ef50c9e2eec44b8eac5b06a92418dc17",
            "b32e29b2cb68420aa4d669cb9a2b2ffb",
            "ee683c346e174410beca98ed2735ee55",
            "f6d58df2e1a040e58562eb95318e67d6",
            "affdbbe0ae944fd8a641043d28172beb",
            "703244bf53014bfea13f7d44aec03e79",
            "eb18fe019d024c75a6c8f1cba43cbf95",
            "50bfc755d4e64c11bdbff34646016f77",
            "77311e8e33b347ccaf6fb70fc5f407da",
            "ecb71901b03d40fb9cd89e7f79cb04b1",
            "ace0b9330c8e432d8c6c20c10991e811",
            "063ab0797f41426a9fb22b751c4ac57c",
            "dff0a0b50e9f4b9abcd9c1be0b8bbe3e",
            "cf263f257719468b809d8917a29a9577",
            "4cb61afc01e54b08bd4d77b3dbae1ec3",
            "9511cff7e15f47478b53eecffd1c9bf2",
            "bed118a2c3c1445ba0d1c1d96b663bc4",
            "dbdaf6fee3de4335a08ba81d3e83707c",
            "c2229290f1754084bad31d1ccc32f90a",
            "093f6ecae5eb4de5a25944bd841a673f",
            "bf9c49770ae442a5bd8a7f5dbf96760a",
            "38c65b368d9144b6b2dafc5c1730b80f",
            "c5dc744ca823411f812374826c882096",
            "6e477ca1b7a94365a5acc4879a3a7834",
            "f85d42af4206413cb94effa604b8048e",
            "3950dae6f6da41e3a384328e28c9c0f1",
            "d9c70df3f28e4b03a6e4ee3d3b96db23",
            "0003dce6f30945158eb2d2a5ed42e5ba",
            "31233299c9fc4e3d9552cae40576c7d7",
            "4eb22ab624b54785a841c6739b1f2901",
            "d3f7f31fe0b84a86ac28de458ac1ee24",
            "b41af30411f14318ae6fd778fb246d5e",
            "bd55d6cb2b9c474e97dac2bc9451be4c",
            "4e480e1505b14b8db29cc04d8e903bea",
            "00f19bc6c025413ab75820560ee7b9a6",
            "4bb7f145a13f421bb98093fe6904e697",
            "a026d19d9884401495f73b997eeafafa"
          ]
        },
        "id": "_WjtqA8TrHcS",
        "outputId": "5aa417db-56d3-4b33-8409-344eae09dc94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a9f66d8b9bc4e1c91b652bec4a76569",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/8.64k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "affdbbe0ae944fd8a641043d28172beb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/11.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9511cff7e15f47478b53eecffd1c9bf2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/27.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9c70df3f28e4b03a6e4ee3d3b96db23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/1020868 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from keras.layers import TextVectorization\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "ds = load_dataset('generics_kb',trust_remote_code=True)['train']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p29c4vwvq_9I"
      },
      "source": [
        "### Unprocessed Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrnyxTV6q_9I",
        "outputId": "84d423a1-cefd-46c8-c931-78563741cf25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of textbatch: <class 'dict'>\n",
            "Keys in textbatch: dict_keys(['source', 'term', 'quantifier_frequency', 'quantifier_number', 'generic_sentence', 'score'])\n",
            "\n",
            "\n",
            "Text 1: AA batteries maintain the settings if the power ever goes off.\n",
            "Text 2: Aardvark females appear to come into season once per year.\n",
            "Text 3: Aardvark holes are used by small buck as a resting place to escape the midday sun.\n"
          ]
        }
      ],
      "source": [
        "for textbatch in ds.take(1):\n",
        "    print('Type of textbatch:',type(textbatch))\n",
        "    print('Keys in textbatch:',textbatch.keys())\n",
        "    print('\\n')\n",
        "\n",
        "i = 0\n",
        "for textbatch in ds.take(3):\n",
        "    i += 1\n",
        "    print('Text {}:'.format(i), textbatch['generic_sentence'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAVLfsdc_ej5"
      },
      "source": [
        "Filter row with length greater than 8.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e60b25d8e1834338bd2c4ff0a06f0244",
            "59619a73cea84f19941f8b4cea5d19df",
            "88f46f3b23df440f82a4f309c253f071",
            "8414d2dacad1479383cbc08f8714171e",
            "b04320d26f4249fd935ca2ab3f8314aa",
            "613715e132c04336a9fbd181cc16807c",
            "21b17b77f75f4d23b4625a4a413664f1",
            "8aac0f720c784d618cc7387d2bb0d1c9",
            "5977db878d654ee5b26f2a88dde1aea7",
            "2a543c6dc87a4169b02adffda3a64cf0",
            "cb2a1b558510406799d9d070950b68b5"
          ]
        },
        "id": "iznq8xGNt2Zr",
        "outputId": "05bae0fc-652f-4d2a-a409-6c1df2cd4d5b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e60b25d8e1834338bd2c4ff0a06f0244",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1020868 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ds = ds.filter(lambda row: len(row[\"generic_sentence\"].split(\" \"))>8 )\n",
        "corpus = [ '<start> ' + row['generic_sentence'].replace(\",\",\" <comma>\") + ' <end>' for row in ds ]  # add start and end tokens and replace commas with <comma>\n",
        "corpus = np.array(corpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9qZ8Y2cq_9J"
      },
      "source": [
        "### Data visualization in Unprocessed Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAAK17Yiq_9J",
        "outputId": "90789568-f130-4167-c353-93e9c4e612dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First sentence in corpus: <start> AA batteries maintain the settings if the power ever goes off. <end>\n"
          ]
        }
      ],
      "source": [
        "print('First sentence in corpus:',corpus[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyYpXLCF_ldR"
      },
      "source": [
        "Create a tokenizer and Detokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "T-bE2JpVbU9E"
      },
      "outputs": [],
      "source": [
        "# tokenizer transforms the text into integers to be fed into the model, and applies padding to make all the sequences the same length\n",
        "tokenizer=TextVectorization( max_tokens=10000, standardize=\"lower_and_strip_punctuation\", encoding=\"utf-8\",) #con il max prende le piu frequenti. ordina i token del vocab dal piu frequente al meno frequente\n",
        "\n",
        " # learn the vocabulary from the corpus and preprocess the text\n",
        "tokenizer.adapt(corpus)\n",
        "\n",
        "class TextDetokenizer:\n",
        "    def __init__(self, vectorize_layer):\n",
        "        self.vectorize_layer = vectorize_layer\n",
        "        vocab = self.vectorize_layer.get_vocabulary()\n",
        "        self.index_to_word = {index: word for index, word in enumerate(vocab)}\n",
        "\n",
        "    def __detokenize_tokens(self, tokens):\n",
        "        def check_token(t):\n",
        "          if t == 3:                                                              # 3 is the index for the <start> token\n",
        "            s=\"<start>\"\n",
        "          elif t ==2:                                                             # 2 is the index for the <end> token\n",
        "            s=\"<end>\"\n",
        "          elif t ==7:                                                             # 7 is the index for the <comma> token\n",
        "            s=\"<comma>\"\n",
        "          else:\n",
        "            s=self.index_to_word.get(t, '[UNK]')                                  # if key found in dict it returns the value, else it returns '[UNK]'\n",
        "          return s                                                                # 1 is the index of the [UNK] token in the vocabulary\n",
        "\n",
        "        return ' '.join([ check_token(token) for token in tokens if token != 0])  # 0 is the index for padding\n",
        "\n",
        "    def __call__(self, batch_tokens):\n",
        "       return [self.__detokenize_tokens(tokens) for tokens in batch_tokens]\n",
        "\n",
        "\n",
        "\n",
        "detokenizer = TextDetokenizer( tokenizer )\n",
        "sentences = tokenizer( corpus ).numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGA-fdrNq_9J"
      },
      "source": [
        "### Visualizing lenght of Vocabulary together with the first 20 keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-i9XY4Uq_9J",
        "outputId": "e4abfed7-8680-4b52-c0fb-ba4a0f01518f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: ['', '[UNK]', 'end', 'start', 'the', 'of', 'and', 'comma', 'is', 'to'] ['a', 'in', 'are', 'that', 'can', 'for', 'or', 'as', 'have', 'with']\n",
            "Vocabulary length: 10000\n"
          ]
        }
      ],
      "source": [
        "print('Vocabulary:',tokenizer.get_vocabulary()[:10],tokenizer.get_vocabulary()[10:20])\n",
        "print('Vocabulary length:',tokenizer.vocabulary_size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ64sns1_pSK"
      },
      "source": [
        "Remove from corpus the sentences where any unknow word appears"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2LPQtryQz5wh"
      },
      "outputs": [],
      "source": [
        "mask = np.sum( (sentences==1) , axis=1) >= 1                # sentences == 1 returns a boolean array with True where the token is the [UNK] token\n",
        "                                                            # creating a mask with True where the sentence has at least one [UNK] token\n",
        "original_data = np.delete( sentences, mask , axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYfOscVk7U0r",
        "outputId": "87ccd449-2c7f-48f7-9275-8cac2bf83893"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(241236, 28)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "original_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-U1SnG4q_9J"
      },
      "source": [
        "Each sentence is padded to get a fixed vector size of 28 tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5puiiQ2D_uxa"
      },
      "source": [
        "Shuffle the sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1ZXLkWB6od0R"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, data, batch_size=32, shuffle=True, seed=42):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.on_epoch_end()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.data) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        data_batch = np.array([self.data[k] for k in indexes])\n",
        "        #copy of ordered sequences\n",
        "        result = np.copy(data_batch)\n",
        "        #shuffle only the relevant positions for each batch\n",
        "        for i in range(data_batch.shape[0]):\n",
        "          np.random.shuffle(data_batch[i,1:data_batch[i].argmin() - 1])\n",
        "\n",
        "        return data_batch , result\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.data))\n",
        "        if self.shuffle:\n",
        "            if self.seed is not None:\n",
        "                np.random.seed(self.seed)\n",
        "            np.random.shuffle(self.indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4W2QvkaFq_9K"
      },
      "outputs": [],
      "source": [
        "# Make a random permutation of training and test set\n",
        "np.random.seed(42)\n",
        "# Shuffle the all data\n",
        "shuffled_indices = np.random.permutation(len(original_data))\n",
        "shuffled_data = original_data[shuffled_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NpIlEdxbq_9K"
      },
      "outputs": [],
      "source": [
        "#split the dataset\n",
        "train_generator = DataGenerator(shuffled_data[:220000])\n",
        "test_generator = DataGenerator(shuffled_data[220000:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uNlq1Khx1oH2"
      },
      "outputs": [],
      "source": [
        "#train_generator = DataGenerator(original_data[:220000])\n",
        "#test_generator = DataGenerator(original_data[220000:])\n",
        "x, y = test_generator.__getitem__(1)  # get first batch of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtC7fP3kq_9K"
      },
      "source": [
        "### Visualizing random tokenized sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttSHnRa_q_9K",
        "outputId": "e3f720b2-aa02-449d-e42d-2fb96b6fa025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized original:  [   3   22 9853 1157   49  300   16  101  214   39    4   58    5  728\n",
            "    2    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "\n",
            "Tokenized shuffled:  [   3  300   39    4   16    5  214   58 1157  101   22 9853  728   49\n",
            "    2    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "i = np.random.randint(0, x.shape[0])                        # get a random index from the batch\n",
        "print(\"Tokenized original: \", y[i])\n",
        "print(\"\\nTokenized shuffled: \", x[i])\n",
        "#print('\\nBatch Size:{}. Lenght:{}'.format(x.shape[0],x.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR5xwMOn4E88",
        "outputId": "92e4b381-0e33-4ced-c0b8-44a733f0237c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original - target sequence:  <start> ranchers clear large areas of rainforest to become pastures for their cattle <end>\n",
            "shuffled - input sequence:  <start> large their areas for cattle ranchers rainforest clear pastures become to of <end>\n",
            "\n",
            "\n",
            "original - target sequence:  <start> some earwigs have stripes on the thorax and abdomen <end>\n",
            "shuffled - input sequence:  <start> stripes thorax some and the earwigs on abdomen have <end>\n",
            "\n",
            "\n",
            "original - target sequence:  <start> magnetic manipulation can turn molecules in a liquid into computing such devices <end>\n",
            "shuffled - input sequence:  <start> into in magnetic such a liquid molecules can manipulation computing turn devices <end>\n",
            "\n",
            "\n",
            "original - target sequence:  <start> healthy wetlands means cleaner water <comma> reduced flooding and more places for recreation <end>\n",
            "shuffled - input sequence:  <start> reduced wetlands and recreation for water places healthy cleaner flooding <comma> means more <end>\n",
            "\n",
            "\n",
            "original - target sequence:  <start> market share is the percent share in sales one company controls in a particular market <end>\n",
            "shuffled - input sequence:  <start> company percent share one controls a sales in market is share the particular in market <end>\n",
            "\n",
            "\n",
            "original - target sequence:  <start> face flies spend only a small amount of time on the animal <end>\n",
            "shuffled - input sequence:  <start> of on animal only a the small flies time amount spend face <end>\n",
            "\n",
            "\n",
            "original - target sequence:  <start> organic foods are extremely important in prevention and management of cancer <end>\n",
            "shuffled - input sequence:  <start> extremely management in of foods are prevention and cancer important organic <end>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "detok_x = detokenizer(x)\n",
        "detok_y = detokenizer(y)\n",
        "\n",
        "for i in range(7):\n",
        "  print(\"original - target sequence: \", detok_y[i])\n",
        "  print(\"shuffled - input sequence: \", detok_x[i])\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9Ukpam36V4d",
        "outputId": "e7dce50a-4663-4298-f1fb-e9acee87490e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<start> ranchers clear large areas of rainforest to become pastures for their cattle <end>',\n",
              " '<start> some earwigs have stripes on the thorax and abdomen <end>',\n",
              " '<start> magnetic manipulation can turn molecules in a liquid into computing such devices <end>',\n",
              " '<start> healthy wetlands means cleaner water <comma> reduced flooding and more places for recreation <end>',\n",
              " '<start> market share is the percent share in sales one company controls in a particular market <end>',\n",
              " '<start> face flies spend only a small amount of time on the animal <end>',\n",
              " '<start> organic foods are extremely important in prevention and management of cancer <end>']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "detokenizer(y)[0:7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo8MazCGBTv3"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NOkuO0CfPo"
      },
      "source": [
        "Let s be the source string and p your prediction. The quality of the results will be measured according to the following metric:\n",
        "\n",
        "1.  look for the longest substring w between s and p\n",
        "2.  compute |w|/max(|s|,|p|)\n",
        "\n",
        "If the match is exact, the score is 1.\n",
        "\n",
        "When computing the score, you should NOT consider the start and end tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-aUrdlXDdVf"
      },
      "source": [
        "The longest common substring can be computed with the SequenceMatcher function of difflib, that allows a simple definition of our metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ulpTRdrF_huh"
      },
      "outputs": [],
      "source": [
        "from difflib import SequenceMatcher\n",
        "\n",
        "def score(s,p):\n",
        "  match = SequenceMatcher(None, s, p).find_longest_match()\n",
        "  #print(match.size)\n",
        "  return (match.size/max(len(p),len(s)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtj-xK1aq_9N"
      },
      "source": [
        "### Printing Max Lenght of Sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB2YfjXNExM-"
      },
      "source": [
        "Let's do an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h17C8bVjEwur",
        "outputId": "b63b98d8-2ada-489e-9465-89937efcf8ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "your score is  0.5423728813559322\n"
          ]
        }
      ],
      "source": [
        "original = \"at first henry wanted to be friends with the king of france\"\n",
        "generated = \"henry wanted to be friends with king of france at the first\"\n",
        "\n",
        "print(\"your score is \",score(original,generated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhpM6939q_9N"
      },
      "source": [
        "### Computing scores, meand and variance of randomly shuffled sequences match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RcsCkZTq_9N",
        "outputId": "83667ffe-575a-430b-efab-7587a6bc6b76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean score of Not-Reordered Sequences: 0.1585000396657057\n",
            "Std of Not-Reordered Sequences: 0.03408054917696136\n",
            "Max Score: 0.25842696629213485\n",
            "Min Score: 0.09782608695652174\n"
          ]
        }
      ],
      "source": [
        "scores = []\n",
        "for i in range(len(detok_x)):\n",
        "    orig = detok_y[i]\n",
        "    gen = detok_x[i]\n",
        "    scores.append(score(orig,gen))\n",
        "\n",
        "print('Mean score of Not-Reordered Sequences:',np.mean(scores))\n",
        "print('Std of Not-Reordered Sequences:',np.std(scores))\n",
        "print('Max Score:',np.max(scores))\n",
        "print('Min Score:',np.min(scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BET8GqBvFugR"
      },
      "source": [
        "The score must be computed as an average of at least 3K random examples taken form the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fwo7xj4GBW1"
      },
      "source": [
        "# What to deliver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6uITuxOGHfJ"
      },
      "source": [
        "You are supposed to deliver a single notebook, suitably commented.\n",
        "The notebook should describe a single model, although you may briefly discuss additional attempts you did.\n",
        "\n",
        "The notebook should contain a full trace of the training.\n",
        "Weights should be made available on request.\n",
        "\n",
        "You must also give a clear assesment of the performance of the model, computed with the metric that has been given to you.\n",
        "\n",
        "# Good work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsqnjpiqq_9O"
      },
      "source": [
        "# Model Description:\n",
        "For this project I opted to use a Transformer model, in order to process sequences of data.\n",
        "The choice has been made thinking of the Transformer architecture advantages over a classical LSTM-NN for Sequence to Sequence problems.\n",
        "\n",
        "1. Transformer Models allow for more efficient parallelization during training compared to LSTMs\n",
        "2. While in LSTMs information needs to propagate through each time-stemp, Transformer Models can capture dependencies between tokens regardless of their distance in the sequence, using the Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7RL4d41q_9O"
      },
      "source": [
        "## Encoder-Decoder structure of a Transformer\n",
        "##### Encoder Layer:\n",
        "* Multi-Head Self-Attention Layer\n",
        "* Position-wise fully connected Feed-Forward Neural Network Layer\n",
        "\n",
        "##### Decoder Layer:\n",
        "* Multi-Head Self-Attention Layer\n",
        "* Encoder-Decoder Cross-Attention Layer to focus on relevant parts of the input sequence\n",
        "* Position-wise fully connected Feed-Forward Neural Network Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UzBAAbS7q_9O"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers, Model\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEgagcHkq_9O"
      },
      "source": [
        "## Defining Token and Position Embedding layer\n",
        "Layer to transform integer-encoded tokens in input into dense, continuous vector representations of a specified dimensionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp2gTJWrq_9O"
      },
      "source": [
        "The Embedding layer will be put outside of the Encoder and Decoder layers in order to *share* the same Learnt Embedding space for Both the Encoder and Decoder and thus *reduce* the number of Parameters of the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyC_yN2q_9O"
      },
      "source": [
        "The Token Embedding layers applies a mask to zero tokens to avoid processing Padding tokens. This Mask is propagated to subsequent layers that support it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "E7hWY_gxq_9O"
      },
      "outputs": [],
      "source": [
        "# output dim. (num_seq, seq_len, d_model)\n",
        "# d_model = dimensionality of the embedding space\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.d_model = d_model\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=d_model, mask_zero=True) # mask_zero=True to avoid processing padding tokens. Mask is propagated to subsequent layers - like attention layer\n",
        "\n",
        "        # create a position embedding learnet during train. Not deterministic, but can lead to capturing positional patterns better for specified data\n",
        "        # embedding has space of max sequence length\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=d_model)\n",
        "\n",
        "    def call(self, x, pos=True):\n",
        "        length = tf.shape(x)[-1]\n",
        "        x = self.token_emb(x)\n",
        "        #x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # scale\n",
        "        if pos:\n",
        "            positions = tf.range(start=0, limit=length, delta=1)                            # holder for the positions of the tokens\n",
        "            positions = self.pos_emb(positions)\n",
        "            #positions = self.positional_encoding(self.d_model)[:, :length,:]\n",
        "            x += positions                                                                  # returns one single output with positions embeddings added to the token embeddings\n",
        "        return x                                                                            # without positional embedding will treat the sentence as a bag of words\n",
        "\n",
        "\n",
        "    # ovveride the compute_mask method to propagate the mask to the next layer by the token embedding layer\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return self.token_emb.compute_mask(inputs, mask)\n",
        "\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "        return pos * angle_rates\n",
        "\n",
        "    def positional_encoding(self, d_model, position=2048):\n",
        "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
        "                                np.arange(d_model)[np.newaxis, :],\n",
        "                                d_model)\n",
        "\n",
        "        # apply sin to even indices in the array; 2i\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "        # apply cos to odd indices in the array; 2i+1\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9rspT58q_9O"
      },
      "source": [
        "## Defining Self-Attention Mechanism for the Transformer\n",
        "##### This Class includes self-attention, causal self-attention and cross-attention layers.\n",
        "##### * For the model to achieve the best results it has been ESSENTIAL to include a Padding Mask to attend only to meaningful tokens. The Padding Mask is computed and propagated by the Embedding Layer.\n",
        "\n",
        "##### * For the Decoder Causal Self-Attention, the Padding Mask has been combined with the Causal Mask. The Causal Mask ensures that during the Self-Attention computation, each position can only attend to Tokens positions preceding the current one.\n",
        "\n",
        "##### * The Cross-Attention Layers has been used in order for the decoder to focus on relevant parts of the Encoder Output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "KR2XO4Ckq_9O"
      },
      "outputs": [],
      "source": [
        "class AttentionMechanism(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0                                                 # verify that the model dimensions are divisible by the number of heads\n",
        "\n",
        "        self.self_mha = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                                  key_dim=d_model//num_heads)\n",
        "        self.causal_mha = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                                    key_dim=d_model//num_heads)\n",
        "        self.cross_mha = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                                   key_dim=d_model//num_heads)\n",
        "\n",
        "        self.layernorm = layers.LayerNormalization(epsilon=1e-6)                        # normalize the output of the multihead attention\n",
        "        #self.dropout = layers.Dropout(dropout_rate)                                    # dropout layer to set random values of attention weights to 0\n",
        "        self.add = layers.Add()                                                         # define the addition layer to add residual connections\n",
        "\n",
        "    def call(self, x, enc_output=None, causal_mask=None, mask=None):#, mask=None):\n",
        "        # causal self-attention\n",
        "        if causal_mask!=None:\n",
        "            if mask!=None:\n",
        "                # Combining Padding Mask with Causal Mask\n",
        "                mask = tf.cast(mask, dtype=tf.int32)\n",
        "                padding_mask = tf.expand_dims(mask, axis=1)# shape: (batch_size, 1, seq_length)\n",
        "                combined_mask = padding_mask * causal_mask  # shape: (batch_size, seq_length, seq_length)\n",
        "                attn_output = self.causal_mha(query=x, key=x, value=x, attention_mask=combined_mask)\n",
        "            else:\n",
        "                attn_output = self.causal_mha(query=x, key=x, value=x, attention_mask=causal_mask)\n",
        "\n",
        "        # cross-attention. Padding mask will be passed by embedding layer\n",
        "        elif enc_output!=None:\n",
        "            attn_output = self.cross_mha(query=x, key=enc_output, value=enc_output)\n",
        "\n",
        "        # self-attention with 0 padding propagated by Embedding layer\n",
        "        else:\n",
        "            attn_output = self.self_mha(query=x, key=x, value=x)\n",
        "\n",
        "        #attn_output = self.dropout(attn_output)                                       # setting random values of attention weights to 0\n",
        "        x = self.add([x, attn_output])                                                 # attention outputs to the input\n",
        "        x = self.layernorm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfpqVbr8q_9O"
      },
      "source": [
        "## Defining the Position Wise Feed-Forward Layer\n",
        "Weights of dense layers have been initialized with GlorotUniform. This weights initialization reduces eventual problems of vanishing or exploding gradients.\n",
        "\n",
        "The attention output is added to the FFN output with a skip connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ZXCGDSQYq_9O"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "        initializer = tf.keras.initializers.GlorotUniform(seed=42)               # initializer for the weights of the FFN layers\n",
        "\n",
        "        # dff first layer dim., from d_model dim. to higher dimensional space to to catpure more complex patterns\n",
        "        # second layer back to d_model dimensions - dimension of each token in the Embedding Dimension\n",
        "        super().__init__()\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(dff, activation='relu', kernel_initializer=initializer),   # first layer of the FFN to increase the dimensionality of the input and capture more complex patterns\n",
        "            layers.Dense(d_model, kernel_initializer=initializer)                   # second layer of the FFN to return to the original dimension\n",
        "            #,layers.Dropout(dropout_rate)                                          \n",
        "        ])\n",
        "        self.dropout = layers.Dropout(dropout_rate)                                 # dropout layer after the FFN\n",
        "\n",
        "        self.add = layers.Add()\n",
        "        self.layernorm = layers.LayerNormalization(epsilon=1e-6)                    # normalize the output of the FFN\n",
        "\n",
        "    # computation that function does when called - ovverriden method of layer class\n",
        "    def call(self, x, training=False):\n",
        "        ffnout = self.ffn(x)                                                        # feed the input to the FFN\n",
        "        ffnout = self.dropout(ffnout, training=training)                            # apply dropout to the output of the FFN\n",
        "        x = self.add([x, ffnout])                                                   # including the residual connection\n",
        "        x = self.layernorm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0dqmE-vq_9O"
      },
      "source": [
        "## Defining and Encoder, with multiple Encoder Layers\n",
        "In input to the Encoder Layer there will be a shuffled sequence.\n",
        "We want the model to learn the relationships between the positions of the tokens in the shuffled sequence and their positions in the original sequence.\n",
        "Position embedding is thus crucial and each encoder layer is composed by a sequence of self-attention and ffn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "7rzPEKXtq_9O"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, dropout_rate =0.1):#maxlen, vocab_size, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        #self.embedding = TokenAndPositionEmbedding(maxlen, vocab_size, d_model)\n",
        "        self.dropout = layers.Dropout(dropout_rate)                                                     # dropout layer to prevent overfitting\n",
        "\n",
        "        self.enc_layers = [ tf.keras.Sequential([\n",
        "                            AttentionMechanism(d_model, num_heads, dropout_rate=dropout_rate),\n",
        "                            PositionWiseFeedForward(d_model, dff) ])                                    # after each pffn a droput layer is added inside the class\n",
        "                           for _ in range(num_layers) ]                                                 # repeat the sequential stack - encoder layers for num_layers times\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        #x = self.embedding(x)                                                                          # in input to the encoder I give Embedded outputs. The embedding layers has been defined outside to share weights with the encoder\n",
        "        x = self.dropout(x, training=training)                                                          # apply dropout layer to the Embedding layer output\n",
        "                                                                                                        # to regularize input representations before feeding them to the encoder layers\n",
        "        for enc_layer in self.enc_layers:                                                               # adding num_layers of encoder layers iterating over list\n",
        "            x = enc_layer(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gVh01kBq_9P"
      },
      "source": [
        "## Defining Decoder Layer\n",
        "##### Each decoder layer will calculate a Causal Mask to pass to self-attention mechanism in order to focus only on past tokens in the sequence, and not peek at the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Afk4B2yGq_9P"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = AttentionMechanism(d_model, num_heads, dropout_rate)\n",
        "        self.ffn = PositionWiseFeedForward(d_model, dff)\n",
        "\n",
        "    def call(self, x, enc_output, training=False):\n",
        "        causal_mask = self.get_causal_attention_mask(x)                         # get the causal mask for the decoder\n",
        "        x = self.attention(x=x, causal_mask=causal_mask)                        # causal self-attention. Causal mask is used to prevent the model from peeking at the future tokens\n",
        "        x = self.attention(x=x, enc_output=enc_output)                          # cross-attention\n",
        "        x = self.ffn(x)                                                         # position-wise feed forward network adds attention outputs to ffn outputs and normalizes the output\n",
        "        return x\n",
        "\n",
        "    # Create lower triangular matrix to be used as a mask for the decoder.\n",
        "    # This mask will be used to prevent the decoder from peeking at the future tokens.\n",
        "    # For sequence of length 4, the mask will look like this:\n",
        "    # Index 1 - [1,0,0,0], Index 2 - [1,1,0,0], Index 3 - [1,1,1,0], Index 4 - [1,1,1,1]\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "\n",
        "        i = tf.range(sequence_length)[:, None]  # create a column vector of shape (sequence_length, 1)\n",
        "        j = tf.range(sequence_length)\n",
        "\n",
        "        mask = tf.cast(i >= j, dtype=tf.int32)  # create a matrix of shape (sequence_length, sequence_length)\n",
        "        mask = tf.reshape(mask, (1, sequence_length, sequence_length))\n",
        "\n",
        "        mask = tf.tile(mask, tf.concat([[batch_size], [1], [1]], axis=0))  # tile the mask to match the shape of the inputs - shape (batch_size, seqlen, seqlen)\n",
        "\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkG4pDaBq_9P"
      },
      "source": [
        "## Defining Decoder as Multiple Stacks of Decoder Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "z0yfR10-q_9P"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, maxlen, dropout_rate=0.1):#vocab_size, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        #self.embedding = TokenAndPositionEmbedding(maxlen, vocab_size, d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, target_seq, enc_output, training=False):\n",
        "        x = self.dropout(target_seq, training=training)                     # apply dropout layer to the Embedding layer output, in input to the decoder\n",
        "                                                                            # The embedding are calculatede in Transformer before this block to share weights with the encoder\n",
        "        # iterating over list of decoder layers\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.dec_layers[i](x, enc_output, training=training)        # passing target sequence and encoder output to the decoder layer. Encoder output is used in cross-attention\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5lPS68Hq_9P"
      },
      "source": [
        "## Putting everything together in Transformer Model\n",
        "##### Token Embedding Space is shared between Encoder and Decoder to use less parameters in the Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "wX6LU7deq_9P"
      },
      "outputs": [],
      "source": [
        "# ovverriding Model class to define the Transformer model\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, maxlen, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.start_token = 3\n",
        "    self.end_token = 2\n",
        "    self.maxlen = maxlen\n",
        "\n",
        "    self.embedding = TokenAndPositionEmbedding(maxlen,\n",
        "                                               vocab_size,\n",
        "                                               d_model)\n",
        "\n",
        "    self.encoder = Encoder(num_layers=num_layers,\n",
        "                           d_model=d_model,\n",
        "                           num_heads=num_heads,\n",
        "                           dff=dff,\n",
        "                           #vocab_size=vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers,\n",
        "                           d_model=d_model,\n",
        "                           num_heads=num_heads,\n",
        "                           dff=dff,\n",
        "                           maxlen=maxlen,\n",
        "                           #vocab_size=vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    input_seq, target_seq = inputs\n",
        "    enc_input = self.embedding(input_seq, pos=True)                       # embedding the input sequences\n",
        "    dec_input = self.embedding(target_seq, pos=True)                      # embedding the target sequences\n",
        "    enc_output = self.encoder(enc_input, training=training)               # passing the embedded input sequences to the encoder\n",
        "    dec_output = self.decoder(dec_input, enc_output, training=training)   # passing the embedded target sequences and encoder output to the decoder\n",
        "    out = self.final(dec_output)                                          # adding a final layer to the decoder to output the token probabilities\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5bkQOzer6yQ"
      },
      "source": [
        "# Setting the parameters and visualizing Model Summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZc9GuKjq_9P",
        "outputId": "d8ac0717-681a-4c8e-af84-0c122844f1c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " token_and_position_embeddi  multiple                  2567168   \n",
            " ng_1 (TokenAndPositionEmbe                                      \n",
            " dding)                                                          \n",
            "                                                                 \n",
            " encoder_1 (Encoder)         multiple                  3159040   \n",
            "                                                                 \n",
            " decoder_1 (Decoder)         multiple                  4211712   \n",
            "                                                                 \n",
            " dense_33 (Dense)            multiple                  2570000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12507920 (47.71 MB)\n",
            "Trainable params: 12507920 (47.71 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "num_layers = 4\n",
        "d_model = 256\n",
        "dff = 1024\n",
        "num_heads = 8\n",
        "vocab_size = tokenizer.vocabulary_size()\n",
        "maxlen = x.shape[1]  # 28\n",
        "dropout_rate = 0.2\n",
        "\n",
        "tblock = Transformer(num_layers=num_layers,\n",
        "                    d_model=d_model,\n",
        "                    num_heads=num_heads,\n",
        "                    dff=dff,\n",
        "                    maxlen=maxlen,\n",
        "                    vocab_size=vocab_size,\n",
        "                    dropout_rate=dropout_rate)\n",
        "\n",
        "# initialize the model with a batch of data to build the model and print the summary\n",
        "x_batch, y_batch = train_generator.__getitem__(1)\n",
        "batch_inputs = (x_batch, y_batch)\n",
        "_  = tblock(batch_inputs)\n",
        "tblock.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIRux_YEFsS9"
      },
      "source": [
        "### The model has around 12.5M parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al_BZzRuq_9P"
      },
      "source": [
        "##### Defining Masked Loss so that only meaningful Tokens contribute - no padding - no start token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-cBB_kflq_9P"
      },
      "outputs": [],
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "    # Create a mask to identify padding tokens (assuming 0 is the padding token)\n",
        "    sos = 3\n",
        "    padding = 0\n",
        "\n",
        "    mask = tf.cast(\n",
        "        tf.math.logical_and(\n",
        "          tf.math.not_equal(y_true, padding),\n",
        "          tf.math.not_equal(y_true, sos)),\n",
        "    dtype=tf.float32\n",
        "    )\n",
        "\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "    # Apply the mask to the loss\n",
        "    loss *= mask\n",
        "\n",
        "    # Compute the average loss, ignoring the padding tokens and start tokens\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ubAnl7JGdMf_"
      },
      "outputs": [],
      "source": [
        "# controlling accuracy without start tokens\n",
        "def masked_accuracy(label, pred):\n",
        "  sos,padding=3,0\n",
        "  pred = tf.argmax(pred, axis=-1)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  same = label == pred\n",
        "\n",
        "  mask = (label!=padding)&(label!=sos)\n",
        "  same = same & mask\n",
        "\n",
        "  same = tf.cast(same, dtype=tf.float64)\n",
        "  mask = tf.cast(mask, dtype=tf.float64)\n",
        "  return tf.reduce_sum(same)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEKCZpFP7eDT"
      },
      "source": [
        "## During training, at each time step, the decoder's input is the ground truth token from the target sequence at the previous time step. Before the first word in target sequence, there is < start > token in the decoder input.\n",
        "\n",
        "##### The dataset has been saved locally in order to obtain the sequence shifted by one position to be given as the decoder target.\n",
        "\n",
        "##### This has been done because the decoder needs to predict next token in the sequence given the current one.\n",
        "\n",
        "Also saving the dataset locally has been a valuable resource-saver for training with Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vV9vaDQPq_9P"
      },
      "outputs": [],
      "source": [
        "# Saving dataset locally\n",
        "def save_ds(train_generator, test_generator):\n",
        "    train_shuff = []  \n",
        "    train_ord = []  \n",
        "\n",
        "    sos = 3\n",
        "    eos = 2\n",
        "    for i in range(len(train_generator)):\n",
        "        data_batch, res = train_generator[i] \n",
        "        train_shuff.extend(data_batch)\n",
        "        train_ord.extend(res)\n",
        "\n",
        "    # Concatenate all batches into a single array\n",
        "    train_ord = tf.convert_to_tensor(np.array(train_ord), dtype=tf.int32)\n",
        "    train_shuff = tf.convert_to_tensor(np.array(train_shuff), dtype=tf.int32)\n",
        "    train_target = train_ord[:, 1:]  # Remove the <start> token from the target sequences - this is becasue the model will predict the next token given the previous tokens\n",
        "    dec_in = train_ord[:,:-1]       # Match lenghts\n",
        "\n",
        "    test_shuff = []  \n",
        "    test_ord = []   \n",
        "    for i in range(len(test_generator)):\n",
        "        data_batch, res = test_generator[i]\n",
        "        test_shuff.extend(data_batch)\n",
        "        test_ord.extend(res)\n",
        "\n",
        "    test_shuff = tf.convert_to_tensor(np.array(test_shuff))\n",
        "    test_ord = tf.convert_to_tensor(np.array(test_ord))\n",
        "\n",
        "    return train_shuff, dec_in, train_target, test_shuff, test_ord\n",
        "\n",
        "train_shuff, dec_in, train_target, test_shuff, test_ord = save_ds(train_generator, test_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0Pu9SUXrOA7"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tj2VueM36df"
      },
      "source": [
        "Using a common learning rate scheduler for Transformers. The learning rate increases during the warmaup period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "gkDgVDo6FFvj"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ky6CZC2U0K6y"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "  def get_config(self):\n",
        "      return {\n",
        "          'd_model': self.d_model.numpy(),\n",
        "          'warmup_steps': self.warmup_steps\n",
        "      }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvEHBFcKFTHR"
      },
      "source": [
        "### In order to obtain the longest substring matching and achieve the best scores, the accuracy of the model has been taken in account more than the mere loss during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "_PT6cqPfoD-J"
      },
      "outputs": [],
      "source": [
        "w_path = '/content/drive/MyDrive/DLExam/Weights/'\n",
        "model_name = 'transformer_teachforce_4_256_1024_8_02.h5'\n",
        "\n",
        "callback1 = ModelCheckpoint(filepath=w_path+model_name, save_best_only=True, save_weights_only=True, monitor='val_masked_accuracy')\n",
        "callback2 = EarlyStopping(monitor='val_masked_accuracy', patience=3, restore_best_weights=True)\n",
        "callbacks = [callback1, callback2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "b2K15dbP0MkG"
      },
      "outputs": [],
      "source": [
        "EPOCHS=16\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi6iRyICgfXr",
        "outputId": "cec01631-7eed-4d6f-a34b-d15e0ab0dccf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "774/774 [==============================] - 124s 147ms/step - loss: 6.4060 - masked_accuracy: 0.2174 - val_loss: 4.6083 - val_masked_accuracy: 0.3787\n",
            "Epoch 2/16\n",
            "774/774 [==============================] - 107s 138ms/step - loss: 3.5947 - masked_accuracy: 0.4845 - val_loss: 2.4019 - val_masked_accuracy: 0.6222\n",
            "Epoch 3/16\n",
            "774/774 [==============================] - 107s 138ms/step - loss: 1.9242 - masked_accuracy: 0.6679 - val_loss: 1.3960 - val_masked_accuracy: 0.7278\n",
            "Epoch 4/16\n",
            "774/774 [==============================] - 107s 138ms/step - loss: 1.2202 - masked_accuracy: 0.7420 - val_loss: 1.0230 - val_masked_accuracy: 0.7666\n",
            "Epoch 5/16\n",
            "774/774 [==============================] - 107s 138ms/step - loss: 0.9589 - masked_accuracy: 0.7710 - val_loss: 0.9233 - val_masked_accuracy: 0.7793\n",
            "Epoch 6/16\n",
            "774/774 [==============================] - 107s 138ms/step - loss: 0.8167 - masked_accuracy: 0.7910 - val_loss: 0.8229 - val_masked_accuracy: 0.7910\n",
            "Epoch 7/16\n",
            "774/774 [==============================] - 106s 138ms/step - loss: 0.6791 - masked_accuracy: 0.8151 - val_loss: 0.7662 - val_masked_accuracy: 0.8066\n",
            "Epoch 8/16\n",
            "774/774 [==============================] - 106s 138ms/step - loss: 0.5806 - masked_accuracy: 0.8348 - val_loss: 0.7142 - val_masked_accuracy: 0.8173\n",
            "Epoch 9/16\n",
            "774/774 [==============================] - 106s 137ms/step - loss: 0.5081 - masked_accuracy: 0.8506 - val_loss: 0.6889 - val_masked_accuracy: 0.8238\n",
            "Epoch 10/16\n",
            "774/774 [==============================] - 107s 138ms/step - loss: 0.4478 - masked_accuracy: 0.8643 - val_loss: 0.6713 - val_masked_accuracy: 0.8278\n",
            "Epoch 11/16\n",
            "774/774 [==============================] - 107s 138ms/step - loss: 0.4004 - masked_accuracy: 0.8759 - val_loss: 0.6622 - val_masked_accuracy: 0.8311\n",
            "Epoch 12/16\n",
            "774/774 [==============================] - 107s 138ms/step - loss: 0.3604 - masked_accuracy: 0.8862 - val_loss: 0.6601 - val_masked_accuracy: 0.8348\n",
            "Epoch 13/16\n",
            "774/774 [==============================] - 107s 138ms/step - loss: 0.3249 - masked_accuracy: 0.8961 - val_loss: 0.6687 - val_masked_accuracy: 0.8363\n",
            "Epoch 14/16\n",
            "774/774 [==============================] - 107s 138ms/step - loss: 0.2956 - masked_accuracy: 0.9040 - val_loss: 0.6673 - val_masked_accuracy: 0.8397\n",
            "Epoch 15/16\n",
            "774/774 [==============================] - 105s 136ms/step - loss: 0.2696 - masked_accuracy: 0.9117 - val_loss: 0.6780 - val_masked_accuracy: 0.8385\n",
            "Epoch 16/16\n",
            "774/774 [==============================] - 106s 137ms/step - loss: 0.2470 - masked_accuracy: 0.9182 - val_loss: 0.6783 - val_masked_accuracy: 0.8409\n"
          ]
        }
      ],
      "source": [
        "tblock.compile(optimizer=optimizer, loss=masked_loss, metrics=masked_accuracy)\n",
        "history = tblock.fit( x=[train_shuff, dec_in], y=train_target, epochs=EPOCHS, batch_size=256, validation_split=0.1, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6mR0BlNmNeK"
      },
      "source": [
        "### To achieve a better Accuracy of the model I have chosen to do 10 more epochs, despite the Validation Loss increasing. Having a better accuracy means more probability that subsequent words are in the correct position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZHz3I_mPCMe",
        "outputId": "2cc7e85d-dec5-48d1-cce5-6701f97042d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "774/774 [==============================] - 106s 137ms/step - loss: 0.2272 - masked_accuracy: 0.9242 - val_loss: 0.6908 - val_masked_accuracy: 0.8433\n",
            "Epoch 2/10\n",
            "774/774 [==============================] - 105s 135ms/step - loss: 0.2095 - masked_accuracy: 0.9298 - val_loss: 0.6877 - val_masked_accuracy: 0.8423\n",
            "Epoch 3/10\n",
            "774/774 [==============================] - 105s 136ms/step - loss: 0.1938 - masked_accuracy: 0.9346 - val_loss: 0.7036 - val_masked_accuracy: 0.8431\n",
            "Epoch 4/10\n",
            "774/774 [==============================] - 106s 137ms/step - loss: 0.1804 - masked_accuracy: 0.9389 - val_loss: 0.7113 - val_masked_accuracy: 0.8439\n",
            "Epoch 5/10\n",
            "774/774 [==============================] - 106s 137ms/step - loss: 0.1676 - masked_accuracy: 0.9431 - val_loss: 0.7201 - val_masked_accuracy: 0.8449\n",
            "Epoch 6/10\n",
            "774/774 [==============================] - 105s 135ms/step - loss: 0.1567 - masked_accuracy: 0.9466 - val_loss: 0.7331 - val_masked_accuracy: 0.8441\n",
            "Epoch 7/10\n",
            "774/774 [==============================] - 106s 137ms/step - loss: 0.1470 - masked_accuracy: 0.9498 - val_loss: 0.7400 - val_masked_accuracy: 0.8450\n",
            "Epoch 8/10\n",
            "774/774 [==============================] - 106s 137ms/step - loss: 0.1381 - masked_accuracy: 0.9528 - val_loss: 0.7514 - val_masked_accuracy: 0.8462\n",
            "Epoch 9/10\n",
            "774/774 [==============================] - 106s 137ms/step - loss: 0.1297 - masked_accuracy: 0.9558 - val_loss: 0.7609 - val_masked_accuracy: 0.8465\n",
            "Epoch 10/10\n",
            "774/774 [==============================] - 106s 137ms/step - loss: 0.1228 - masked_accuracy: 0.9580 - val_loss: 0.7686 - val_masked_accuracy: 0.8472\n"
          ]
        }
      ],
      "source": [
        "EPOCHS_2=10\n",
        "history = tblock.fit( x=[train_shuff, dec_in], y=train_target, epochs=EPOCHS_2, batch_size=256, validation_split=0.1, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_QOBhqKSTge"
      },
      "source": [
        "### Seing increasing results, I opted to maximize even further the val_accuracy, stopping the training with the early stopping callback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzz8jz6GIXNK",
        "outputId": "4cd0b262-43f8-4503-d93d-a2d528a217ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "194/194 [==============================] - 221s 968ms/step - loss: 0.0553 - masked_accuracy: 0.9819 - val_loss: 0.7401 - val_masked_accuracy: 0.8557\n",
            "Epoch 2/30\n",
            "194/194 [==============================] - 187s 962ms/step - loss: 0.0488 - masked_accuracy: 0.9843 - val_loss: 0.7436 - val_masked_accuracy: 0.8567\n",
            "Epoch 3/30\n",
            "194/194 [==============================] - 184s 947ms/step - loss: 0.0451 - masked_accuracy: 0.9857 - val_loss: 0.7513 - val_masked_accuracy: 0.8563\n",
            "Epoch 4/30\n",
            "194/194 [==============================] - 185s 953ms/step - loss: 0.0429 - masked_accuracy: 0.9863 - val_loss: 0.7630 - val_masked_accuracy: 0.8568\n",
            "Epoch 5/30\n",
            "194/194 [==============================] - 184s 947ms/step - loss: 0.0417 - masked_accuracy: 0.9867 - val_loss: 0.7786 - val_masked_accuracy: 0.8559\n",
            "Epoch 6/30\n",
            "194/194 [==============================] - 183s 946ms/step - loss: 0.0412 - masked_accuracy: 0.9867 - val_loss: 0.7877 - val_masked_accuracy: 0.8553\n",
            "Epoch 7/30\n",
            "194/194 [==============================] - 184s 947ms/step - loss: 0.0420 - masked_accuracy: 0.9863 - val_loss: 0.8008 - val_masked_accuracy: 0.8548\n"
          ]
        }
      ],
      "source": [
        "EPOCHS_3 = 30\n",
        "tblock.compile(optimizer=optimizer, loss=masked_loss, metrics=masked_accuracy)\n",
        "history = tblock.fit( x=[train_shuff, dec_in], y=train_target, epochs=EPOCHS_3, batch_size=1024, validation_split=0.1, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqVKqnBHO79Q"
      },
      "source": [
        "# Just a Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqGmWWY7an4R",
        "outputId": "485b3811-f6b5-4ed3-eddf-8dfbfe69332d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sequence: ['<start> at many of history levels interactions through emerges a form hierarchical <end>']\n",
            "Target sequence: ['<start> form emerges through a history of interactions at many hierarchical levels <end>']\n",
            "Predicted sequence: ['<start> history emerges at many levels of interactions through a hierarchical form <end>']\n",
            "\n",
            "\n",
            "Input sequence: ['<start> blood synthesis is of necessary acid red of formation folic acids and the cells the for nucleic <end>']\n",
            "Target sequence: ['<start> folic acid is necessary for the synthesis of nucleic acids and the formation of red blood cells <end>']\n",
            "Predicted sequence: ['<start> folic acid is necessary for the formation of the red blood cells and the synthesis of nucleic <end>']\n",
            "\n",
            "\n",
            "Input sequence: ['<start> <comma> combinations <comma> pictures and <comma> sounds have of a websites graphics words <end>']\n",
            "Target sequence: ['<start> websites have a combinations of words <comma> graphics <comma> pictures <comma> and sounds <end>']\n",
            "Predicted sequence: ['<start> websites have a combinations of words <comma> graphics <comma> pictures <comma> and sounds <end>']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def predict(input_sequence, maxlen=28, model=tblock):\n",
        "    sos = 3\n",
        "    eos = 2\n",
        "    batch_size = input_sequence.shape[0]\n",
        "\n",
        "    generated = tf.expand_dims([sos], 0)   # Add batch dimension and initialize with start token\n",
        "    for i in range(maxlen):\n",
        "        predictions = model([input_sequence, generated], training=False)   # Pass through the model\n",
        "        predicted_id = tf.argmax(predictions[:,-1,:], axis=-1).numpy()      # Get the token with highest probability for last time-sequence\n",
        "        new_token = predicted_id[0].item()\n",
        "\n",
        "        # Update the decoder input with the predicted token for the next iteration\n",
        "        generated = tf.concat([generated, tf.expand_dims([new_token], 0)], axis=-1)\n",
        "        if predicted_id == eos:  # If end token is predicted, stop\n",
        "            return np.array(generated)\n",
        "\n",
        "\n",
        "    return np.array(generated)\n",
        "\n",
        "for _ in range(3):\n",
        "    i = np.random.randint(0, test_shuff.shape[0])                                   # get a random index from the batch\n",
        "    pred = predict(test_shuff[i:i+1], maxlen=maxlen, model=tblock)                  # get the model's prediction for the input sequence\n",
        "    print('Input sequence:', detokenizer(np.array(test_shuff[i:i+1])))              # print the input sequence\n",
        "    print('Target sequence:', detokenizer(np.array(test_ord[i:i+1])))               # print the target sequence\n",
        "    print('Predicted sequence:', detokenizer((pred[0:1])))                          # print the predicted sequence\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2EO5gJwrGA_"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5yyfIz8FmDh"
      },
      "source": [
        "### To calculate the scores, sentences are taken from the < start >, till the < end > tokens, not included."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "r6FLbWIK8cDE"
      },
      "outputs": [],
      "source": [
        "def predict_batches(input_sequence, maxlen=28, model=None):\n",
        "    sos = 3\n",
        "    eos = 2\n",
        "    batch_size = input_sequence.shape[0]\n",
        "\n",
        "    generated = tf.cast(tf.constant([[sos]] * batch_size),dtype=tf.int64)   # initialize with start token for each batch element\n",
        "    for i in range(maxlen):\n",
        "        predictions = model([input_sequence, generated], training=False)    # pass through the model\n",
        "        predicted_ids = tf.argmax(predictions[:, -1, :], axis=-1).numpy()   # get the token with highest probability for last time-sequence\n",
        "        new_tokens = tf.expand_dims(predicted_ids, axis=1)                  # eeshape to (batch_size, 1)\n",
        "\n",
        "        generated = tf.concat([generated, new_tokens], axis=-1)             # concatenate new tokens\n",
        "\n",
        "        if np.all(predicted_ids == eos):\n",
        "            break\n",
        "\n",
        "    return np.array(generated)\n",
        "\n",
        "\n",
        "scores = []\n",
        "def calculate_scores(model, predict, test_shuff, test_ord, detokenizer, scoring, n=None, batch_size=32, prints=-1):\n",
        "    if n is None:\n",
        "        n = test_shuff.shape[0]\n",
        "\n",
        "    sos, eos = 3, 2\n",
        "    #assert n>=batch_size\n",
        "    for i in range(0, n, batch_size):\n",
        "        batch_end = min(i + batch_size, n)\n",
        "        preds = predict(test_shuff[i:batch_end], model=model)\n",
        "        for j in range(preds.shape[0]):\n",
        "            if(preds[j:j+1].shape[1]!=0):\n",
        "                pred_sindx = np.where(preds[j] == sos)[0]\n",
        "                pred_eindx = np.where(preds[j] == eos)[0]\n",
        "\n",
        "                if len(pred_sindx) == 0 or len(pred_eindx) == 0:\n",
        "                    continue\n",
        "\n",
        "                pred_sindx = pred_sindx[0]\n",
        "                pred_eindx = pred_eindx[0]\n",
        "\n",
        "                orderered = test_ord[i + j:i + j + 1].numpy()\n",
        "                ord_sindx = np.where(orderered[0] == sos)[0]\n",
        "                ord_eindx = np.where(orderered[0] == eos)[0]\n",
        "                \n",
        "                if len(ord_sindx) == 0 or len(ord_eindx) == 0:\n",
        "                    continue\n",
        "\n",
        "                ord_sindx = ord_sindx[0]\n",
        "                ord_eindx = ord_eindx[0]\n",
        "\n",
        "                orig = detokenizer(orderered[0:1, ord_sindx+1:ord_eindx])\n",
        "                gen = detokenizer(preds[j:j + 1, pred_sindx+1:pred_eindx])\n",
        "                scores.append(scoring(orig[0], gen[0]))\n",
        "\n",
        "            if i + j < prints:\n",
        "                print(f'Original_{i + j + 1}:', orig[0])\n",
        "                print(f'Generated_{i + j + 1}:', gen[0])\n",
        "                print(f'Score_{i + j + 1}:', scores[i + j])\n",
        "                print('\\n')\n",
        "\n",
        "    mean = np.mean(scores)\n",
        "    std = np.std(scores)\n",
        "\n",
        "    return scores,mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Imq5pD2JmGdb",
        "outputId": "2a70999e-8a9b-480b-c34f-216e604d3efb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original_1: recycling prevents pollution and helps conserve precious natural resources\n",
            "Generated_1: recycling helps conserve precious natural resources and prevents pollution\n",
            "Score_1: 0.5675675675675675\n",
            "\n",
            "\n",
            "Original_2: predators like to eat banana slugs at all stages of their lives\n",
            "Generated_2: predators like to eat banana slugs at all stages of their lives\n",
            "Score_2: 1.0\n",
            "\n",
            "\n",
            "Original_3: corporate profits drive stock prices and corporations have been getting serious about business\n",
            "Generated_3: corporations have serious profits about getting stock and corporate profits before business prices\n",
            "Score_3: 0.1836734693877551\n",
            "\n",
            "\n",
            "Original_4: snails are hermaphrodites but they have to mate before laying some days later\n",
            "Generated_4: some snails have hermaphrodites but they are laying to mate later days before\n",
            "Score_4: 0.33766233766233766\n",
            "\n",
            "\n",
            "Original_5: abuse can affect any family regardless of income <comma> profession <comma> religion <comma> or education\n",
            "Generated_5: abuse can affect any profession <comma> regardless of family <comma> education <comma> or income religion\n",
            "Score_5: 0.20952380952380953\n",
            "\n",
            "\n",
            "Model tested on 21216 elements of test-set:\n",
            "\n",
            "Mean value of scorings: 0.5488481903678519\n"
          ]
        }
      ],
      "source": [
        "scores=[]                   # reset the scores\n",
        "N_EL = test_shuff.shape[0]  # number of elements to test\n",
        "PRINT_PREDS = 5             # print the first n predictions\n",
        "PRED_BATCH = 1024\n",
        "\n",
        "scores, mean, std = calculate_scores(tblock, predict_batches, test_shuff, test_ord, detokenizer, score, n=N_EL, batch_size=PRED_BATCH, prints=PRINT_PREDS)\n",
        "\n",
        "print('Model tested on {} elements of test-set:'.format(N_EL))\n",
        "print('\\nMean value of scorings:', mean)\n",
        "#print('Std of scorings:', std)\n",
        "#print('Max. score:', np.max(scores))\n",
        "#print('Min. score:', np.min(scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQqk3JQ6cbCA"
      },
      "source": [
        "# The Model Achieves an Average Score of 0.548848"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7AQChefoDib"
      },
      "source": [
        "## Different Models and Approaches:\n",
        "### Other models have been tested to reduce the number of the parameters.\n",
        "### For example a good model has been obtained reducing the width of the model (parameters d_model=128 and dff=512) and increasing the depth to 10 layers. This model has a total of around 7M paramets, and achieves a good score of 0.5125.\n",
        "### Another model achieving good results that has been tested is the one with same width proposed here, but with more depth and an increased number of layers. In order to not increase too much the number of parameters though, I have chosen to stick to 4 layers for the encoder and decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez11nsjwoB_S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0003dce6f30945158eb2d2a5ed42e5ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b41af30411f14318ae6fd778fb246d5e",
            "placeholder": "​",
            "style": "IPY_MODEL_bd55d6cb2b9c474e97dac2bc9451be4c",
            "value": "Generating train split: 100%"
          }
        },
        "00f19bc6c025413ab75820560ee7b9a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "063ab0797f41426a9fb22b751c4ac57c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "093f6ecae5eb4de5a25944bd841a673f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bc54c4aac844ed798eefe7e8492beb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21b17b77f75f4d23b4625a4a413664f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24f8da3ee602482b8305a056f636cbc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee683c346e174410beca98ed2735ee55",
            "placeholder": "​",
            "style": "IPY_MODEL_f6d58df2e1a040e58562eb95318e67d6",
            "value": " 8.64k/8.64k [00:00&lt;00:00, 156kB/s]"
          }
        },
        "291a6bd34c5146a9860516a5ed510ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a543c6dc87a4169b02adffda3a64cf0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31233299c9fc4e3d9552cae40576c7d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e480e1505b14b8db29cc04d8e903bea",
            "max": 1020868,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00f19bc6c025413ab75820560ee7b9a6",
            "value": 1020868
          }
        },
        "38c65b368d9144b6b2dafc5c1730b80f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3950dae6f6da41e3a384328e28c9c0f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bb7f145a13f421bb98093fe6904e697": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cb61afc01e54b08bd4d77b3dbae1ec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e480e1505b14b8db29cc04d8e903bea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eb22ab624b54785a841c6739b1f2901": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bb7f145a13f421bb98093fe6904e697",
            "placeholder": "​",
            "style": "IPY_MODEL_a026d19d9884401495f73b997eeafafa",
            "value": " 1020868/1020868 [01:54&lt;00:00, 10402.65 examples/s]"
          }
        },
        "50bfc755d4e64c11bdbff34646016f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf263f257719468b809d8917a29a9577",
            "placeholder": "​",
            "style": "IPY_MODEL_4cb61afc01e54b08bd4d77b3dbae1ec3",
            "value": " 11.9k/11.9k [00:00&lt;00:00, 337kB/s]"
          }
        },
        "59619a73cea84f19941f8b4cea5d19df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_613715e132c04336a9fbd181cc16807c",
            "placeholder": "​",
            "style": "IPY_MODEL_21b17b77f75f4d23b4625a4a413664f1",
            "value": "Filter: 100%"
          }
        },
        "5977db878d654ee5b26f2a88dde1aea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "613715e132c04336a9fbd181cc16807c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6706190e612440cda288e23851417305": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bc54c4aac844ed798eefe7e8492beb6",
            "placeholder": "​",
            "style": "IPY_MODEL_291a6bd34c5146a9860516a5ed510ef0",
            "value": "Downloading builder script: 100%"
          }
        },
        "6a8f063cf8d549379a5d6c2fcb946b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef50c9e2eec44b8eac5b06a92418dc17",
            "max": 8643,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b32e29b2cb68420aa4d669cb9a2b2ffb",
            "value": 8643
          }
        },
        "6e477ca1b7a94365a5acc4879a3a7834": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "703244bf53014bfea13f7d44aec03e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecb71901b03d40fb9cd89e7f79cb04b1",
            "placeholder": "​",
            "style": "IPY_MODEL_ace0b9330c8e432d8c6c20c10991e811",
            "value": "Downloading readme: 100%"
          }
        },
        "77311e8e33b347ccaf6fb70fc5f407da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8414d2dacad1479383cbc08f8714171e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a543c6dc87a4169b02adffda3a64cf0",
            "placeholder": "​",
            "style": "IPY_MODEL_cb2a1b558510406799d9d070950b68b5",
            "value": " 1020868/1020868 [00:21&lt;00:00, 29735.31 examples/s]"
          }
        },
        "86ffb50458df45f3a9b924430cecaf66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88f46f3b23df440f82a4f309c253f071": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aac0f720c784d618cc7387d2bb0d1c9",
            "max": 1020868,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5977db878d654ee5b26f2a88dde1aea7",
            "value": 1020868
          }
        },
        "8aac0f720c784d618cc7387d2bb0d1c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9511cff7e15f47478b53eecffd1c9bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bed118a2c3c1445ba0d1c1d96b663bc4",
              "IPY_MODEL_dbdaf6fee3de4335a08ba81d3e83707c",
              "IPY_MODEL_c2229290f1754084bad31d1ccc32f90a"
            ],
            "layout": "IPY_MODEL_093f6ecae5eb4de5a25944bd841a673f"
          }
        },
        "9a9f66d8b9bc4e1c91b652bec4a76569": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6706190e612440cda288e23851417305",
              "IPY_MODEL_6a8f063cf8d549379a5d6c2fcb946b22",
              "IPY_MODEL_24f8da3ee602482b8305a056f636cbc4"
            ],
            "layout": "IPY_MODEL_86ffb50458df45f3a9b924430cecaf66"
          }
        },
        "a026d19d9884401495f73b997eeafafa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ace0b9330c8e432d8c6c20c10991e811": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "affdbbe0ae944fd8a641043d28172beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_703244bf53014bfea13f7d44aec03e79",
              "IPY_MODEL_eb18fe019d024c75a6c8f1cba43cbf95",
              "IPY_MODEL_50bfc755d4e64c11bdbff34646016f77"
            ],
            "layout": "IPY_MODEL_77311e8e33b347ccaf6fb70fc5f407da"
          }
        },
        "b04320d26f4249fd935ca2ab3f8314aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b32e29b2cb68420aa4d669cb9a2b2ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b41af30411f14318ae6fd778fb246d5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd55d6cb2b9c474e97dac2bc9451be4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bed118a2c3c1445ba0d1c1d96b663bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf9c49770ae442a5bd8a7f5dbf96760a",
            "placeholder": "​",
            "style": "IPY_MODEL_38c65b368d9144b6b2dafc5c1730b80f",
            "value": "Downloading data: 100%"
          }
        },
        "bf9c49770ae442a5bd8a7f5dbf96760a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2229290f1754084bad31d1ccc32f90a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f85d42af4206413cb94effa604b8048e",
            "placeholder": "​",
            "style": "IPY_MODEL_3950dae6f6da41e3a384328e28c9c0f1",
            "value": " 27.1M/27.1M [00:00&lt;00:00, 36.8MB/s]"
          }
        },
        "c5dc744ca823411f812374826c882096": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb2a1b558510406799d9d070950b68b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf263f257719468b809d8917a29a9577": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3f7f31fe0b84a86ac28de458ac1ee24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9c70df3f28e4b03a6e4ee3d3b96db23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0003dce6f30945158eb2d2a5ed42e5ba",
              "IPY_MODEL_31233299c9fc4e3d9552cae40576c7d7",
              "IPY_MODEL_4eb22ab624b54785a841c6739b1f2901"
            ],
            "layout": "IPY_MODEL_d3f7f31fe0b84a86ac28de458ac1ee24"
          }
        },
        "dbdaf6fee3de4335a08ba81d3e83707c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5dc744ca823411f812374826c882096",
            "max": 27147920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e477ca1b7a94365a5acc4879a3a7834",
            "value": 27147920
          }
        },
        "dff0a0b50e9f4b9abcd9c1be0b8bbe3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e60b25d8e1834338bd2c4ff0a06f0244": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59619a73cea84f19941f8b4cea5d19df",
              "IPY_MODEL_88f46f3b23df440f82a4f309c253f071",
              "IPY_MODEL_8414d2dacad1479383cbc08f8714171e"
            ],
            "layout": "IPY_MODEL_b04320d26f4249fd935ca2ab3f8314aa"
          }
        },
        "eb18fe019d024c75a6c8f1cba43cbf95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_063ab0797f41426a9fb22b751c4ac57c",
            "max": 11863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dff0a0b50e9f4b9abcd9c1be0b8bbe3e",
            "value": 11863
          }
        },
        "ecb71901b03d40fb9cd89e7f79cb04b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee683c346e174410beca98ed2735ee55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef50c9e2eec44b8eac5b06a92418dc17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6d58df2e1a040e58562eb95318e67d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f85d42af4206413cb94effa604b8048e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
