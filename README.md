# Transformer_Seq2Seq
### Model Description: Implementation of the Transformer Model Architecture in Keras.

‚úèÔ∏è Deep Learning Problem: The purpose of this project is to take in input a sequence of words corresponding to a random permutation of a given english sentence, and reconstruct the original sentence.
The choice of the Transformer model as the atchitecture to use has been done especially for the ability to find correlations on tokens independently from their position in the sequence, unlike LSTM based NNs.

üî¥ Problem Category: seq2seq problem.
